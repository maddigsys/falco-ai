#!/bin/bash

echo "ğŸ” Checking Ollama Status..."
echo "================================"

# Check if Ollama is running
if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "âœ… Ollama service is running"
    
    # Get available models
    echo -e "\nğŸ“¦ Available models:"
    MODELS=$(curl -s http://localhost:11434/api/tags | jq -r '.models[]?.name // empty' 2>/dev/null)
    
    if [ -z "$MODELS" ]; then
        echo "âŒ No models downloaded yet"
        echo ""
        echo "ğŸ”„ To download the default model, run:"
        echo "curl -X POST http://localhost:11434/api/pull -H 'Content-Type: application/json' -d '{\"name\": \"tinyllama\"}'"
        echo ""
        echo "ğŸ”’ Or use cybersecurity model for advanced analysis:"
        echo "curl -X POST http://localhost:11434/api/pull -H 'Content-Type: application/json' -d '{\"name\": \"jimscard/whiterabbit-neo:latest\"}'"
    else
        echo "âœ… Downloaded models:"
        echo "$MODELS" | while read -r model; do
            echo "  â€¢ $model"
        done
    fi
    
    echo -e "\nğŸŒ Ollama API accessible at: http://localhost:11434"
    echo "ğŸ”— Web UI AI Config: http://localhost:8080/config/ai"
    
else
    echo "âŒ Ollama service is not responding"
    echo "ğŸš€ Try restarting with: docker-compose restart ollama"
fi

echo -e "\n================================" 