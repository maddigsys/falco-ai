version: '3.8'

services:
  # Main Falco AI Alert System
  falco-ai-alerts:
    # Option 1: Use published image (recommended for production)
    # image: maddigsys/falco-ai-alerts:v1.5.7
    # Option 2: Build locally (for development with new features)
    build: .
    image: maddigsys/falco-ai-alerts:weaviate-dev
    ports:
      - "8080:8080"  # Main webhook and Web UI port
    environment:
      # Core Configuration
      - FALCO_AI_PORT=8080
      - WEB_UI_ENABLED=true
      - LOG_LEVEL=INFO
      
      # AI Provider Configuration (defaults to Ollama for local testing)
      - PROVIDER_NAME=${PROVIDER_NAME:-ollama}
      - MODEL_NAME=${MODEL_NAME:-tinyllama}
      
      # Cloud AI Providers (optional)
      - PORTKEY_API_KEY=${PORTKEY_API_KEY:-}
      - OPENAI_VIRTUAL_KEY=${OPENAI_VIRTUAL_KEY:-}
      - GEMINI_VIRTUAL_KEY=${GEMINI_VIRTUAL_KEY:-}
      
      # Ollama Configuration (for local testing)
      - OLLAMA_API_URL=http://ollama:11434/api/generate
      - OLLAMA_MODEL_NAME=${OLLAMA_MODEL_NAME:-tinyllama}
      
      # Slack Configuration (optional)
      - SLACK_BOT_TOKEN=${SLACK_BOT_TOKEN:-}
      - SLACK_CHANNEL_NAME=${SLACK_CHANNEL_NAME:-#security-alerts}
      
      # Alert Configuration
      - MIN_PRIORITY=${MIN_PRIORITY:-warning}
      - IGNORE_OLDER=${IGNORE_OLDER:-1}
      
      # Weaviate Configuration
      - WEAVIATE_ENABLED=${WEAVIATE_ENABLED:-true}
      - WEAVIATE_HOST=weaviate
      - WEAVIATE_PORT=8080
      - WEAVIATE_GRPC_PORT=50051
    volumes:
      - ./templates:/app/templates:ro
      - ./app.py:/app/app.py:ro  # Mount updated app.py for fixes
      - alerts_data:/app/data  # Persistent storage for database and other data
    depends_on:
      - ollama
      - weaviate
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s  # Give Ollama time to start
    networks:
      - falco-network

  # Falco Security Runtime (Linux only - use profile to enable)
  falco:
    image: falcosecurity/falco:latest
    container_name: falco
    privileged: true
    volumes:
      - /var/run/docker.sock:/host/var/run/docker.sock:ro
      - /dev:/host/dev:ro
      - /proc:/host/proc:ro
      - /boot:/host/boot:ro
      - /lib/modules:/host/lib/modules:ro
      - /usr:/host/usr:ro
      - /etc:/host/etc:ro
      - ./falco-config.yaml:/etc/falco/falco.yaml:ro
    environment:
      - FALCO_GRPC_ENABLED=false
    command: ["/usr/bin/falco", "--pidfile=/var/run/falco.pid"]
    depends_on:
      - falco-ai-alerts
    restart: unless-stopped
    networks:
      - falco-network
    profiles:
      - falco-linux  # Only run on Linux systems

  # Ollama Local LLM Service (included by default for local testing)
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - falco-network
    # Just serve Ollama - model will be pulled on first use
    command: serve

  # Weaviate Vector Database for AI-Enhanced Search
  weaviate:
    image: semitechnologies/weaviate:1.25.0
    ports:
      - "8082:8080"  # HTTP API
      - "50051:50051"  # gRPC API
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'text2vec-openai'
      ENABLE_MODULES: 'text2vec-openai,text2vec-huggingface,text2vec-ollama,qna-openai,generative-openai,generative-ollama'
      CLUSTER_HOSTNAME: 'node1'
      LOG_LEVEL: 'info'
      # OpenAI Configuration (passed through from environment)
      OPENAI_APIKEY: ${OPENAI_API_KEY:-}
    volumes:
      - weaviate_data:/var/lib/weaviate
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/.well-known/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - falco-network

  # Optional: Adminer for database management (development only)
  adminer:
    image: adminer:latest
    ports:
      - "8081:8080"
    environment:
      - ADMINER_DEFAULT_SERVER=sqlite:///app/data/alerts.db
    volumes:
      - alerts_data:/app/data:ro
    profiles:
      - dev
    networks:
      - falco-network

  # Optional: Prometheus for monitoring (development/testing)
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    profiles:
      - monitoring
    networks:
      - falco-network

networks:
  falco-network:
    driver: bridge

volumes:
  alerts_data:
    driver: local
  ollama_data:
    driver: local
  weaviate_data:
    driver: local