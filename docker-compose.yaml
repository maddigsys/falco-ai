version: '3.8'

services:
  # Main Falco AI Alert System
  falco-ai-alerts:
    # Option 1: Use published image (recommended for production)
    image: maddigsys/falco-ai-alerts:latest
    # Option 2: Build locally (for development with new features)
    # build: .
    # image: maddigsys/falco-ai-alerts:dev
    ports:
      - "8080:8080"  # Main webhook and Web UI port
    environment:
      # Core Configuration
      - FALCO_AI_PORT=8080
      - WEB_UI_ENABLED=true
      - LOG_LEVEL=INFO
      
      # AI Provider Configuration (defaults to Ollama for local testing)
      - PROVIDER_NAME=${PROVIDER_NAME:-ollama}
      - MODEL_NAME=${MODEL_NAME:-tinyllama}
      
      # Cloud AI Providers (optional)
      - PORTKEY_API_KEY=${PORTKEY_API_KEY:-}
      - OPENAI_VIRTUAL_KEY=${OPENAI_VIRTUAL_KEY:-}
      - GEMINI_VIRTUAL_KEY=${GEMINI_VIRTUAL_KEY:-}
      
      # Ollama Configuration (for local testing)
      - OLLAMA_API_URL=http://ollama:11434/api/generate
      - OLLAMA_MODEL_NAME=${OLLAMA_MODEL_NAME:-tinyllama}
      
      # Slack Configuration (optional)
      - SLACK_BOT_TOKEN=${SLACK_BOT_TOKEN:-}
      - SLACK_CHANNEL_NAME=${SLACK_CHANNEL_NAME:-#security-alerts}
      
      # Alert Configuration
      - MIN_PRIORITY=${MIN_PRIORITY:-warning}
      - IGNORE_OLDER=${IGNORE_OLDER:-1}
      
      # Weaviate Configuration
      - WEAVIATE_ENABLED=${WEAVIATE_ENABLED:-true}
      - WEAVIATE_HOST=weaviate
      - WEAVIATE_PORT=8080
      - WEAVIATE_GRPC_PORT=50051
    volumes:
      - alerts_data:/app/data  # Persistent storage for database and other data
    depends_on:
      - ollama
      - weaviate
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s  # Give Ollama time to start
    networks:
      - falco-network

  # Event Generator - creates test security events for development
  event-generator:
    image: alpine:latest
    container_name: event-generator
    volumes:
      - ./generate_events.py:/app/generate_events.py:ro
    environment:
      - WEBHOOK_URL=http://falco-ai-alerts:8080/falco-webhook
      - GENERATION_INTERVAL=30
    depends_on:
      - falco-ai-alerts
    restart: unless-stopped
    networks:
      - falco-network
    command: >
      sh -c "
        apk add --no-cache python3 py3-requests curl &&
        echo 'Starting event generator...' &&
        python3 /app/generate_events.py
      "
    profiles:
      - events  # Use profile to enable: docker compose --profile events up

  # Ollama Local LLM Service (included by default for local testing)
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_REQUEST_TIMEOUT=30
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - falco-network
    # Just serve Ollama - model will be pulled on first use
    command: serve

  # Weaviate Vector Database for AI-Enhanced Search
  weaviate:
    image: semitechnologies/weaviate:1.25.0
    ports:
      - "8082:8080"  # HTTP API
      - "50051:50051"  # gRPC API
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'text2vec-openai'
      ENABLE_MODULES: 'text2vec-openai,text2vec-huggingface,text2vec-ollama,qna-openai,generative-openai,generative-ollama'
      CLUSTER_HOSTNAME: 'node1'
      LOG_LEVEL: 'info'
      # OpenAI Configuration (passed through from environment)
      OPENAI_APIKEY: ${OPENAI_API_KEY:-}
    volumes:
      - weaviate_data:/var/lib/weaviate
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/.well-known/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - falco-network

  # Optional: Adminer for database management (development only)
  adminer:
    image: adminer:latest
    ports:
      - "8081:8080"
    environment:
      - ADMINER_DEFAULT_SERVER=sqlite:///app/data/alerts.db
    volumes:
      - alerts_data:/app/data:ro
    profiles:
      - dev
    networks:
      - falco-network

  # Optional: Prometheus for monitoring (development/testing)
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    profiles:
      - monitoring
    networks:
      - falco-network

networks:
  falco-network:
    driver: bridge

volumes:
  alerts_data:
    driver: local
  ollama_data:
    driver: local
  weaviate_data:
    driver: local