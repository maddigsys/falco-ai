apiVersion: batch/v1
kind: Job
metadata:
  name: ollama-model-init
  namespace: falco-ai-alerts
  labels:
    app.kubernetes.io/name: ollama-init
    app.kubernetes.io/component: initialization
    app.kubernetes.io/part-of: falco-ecosystem
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ollama-init
        app.kubernetes.io/component: initialization
    spec:
      restartPolicy: OnFailure
      initContainers:
      - name: wait-for-ollama
        image: busybox:1.35
        command:
        - sh
        - -c
        - |
          echo "Waiting for Ollama service to be ready..."
          until wget -q --spider http://OLLAMA_SERVICE_NAME:11434/api/tags; do
            echo "Ollama not ready, waiting 10 seconds..."
            sleep 10
          done
          echo "Ollama is ready!"
      containers:
      - name: model-puller
        image: curlimages/curl:8.5.0
        command:
        - sh
        - -c
        - |
          echo "Pulling default model: llama3.1:7b"
          curl -X POST http://OLLAMA_SERVICE_NAME:11434/api/pull \
            -H "Content-Type: application/json" \
            -d '{"name": "llama3.1:7b"}' \
            --max-time 1800
          
          echo "Verifying model was pulled..."
          curl -s http://OLLAMA_SERVICE_NAME:11434/api/tags | grep -q "llama3.1" || exit 1
          
          echo "Model initialization complete!"
        resources:
          requests:
            memory: "64Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "200m"
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000 