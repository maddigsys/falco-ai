apiVersion: batch/v1
kind: Job
metadata:
  name: ollama-model-init
  namespace: falco-ai-alerts
  labels:
    app.kubernetes.io/name: ollama-init
    app.kubernetes.io/component: initialization
    app.kubernetes.io/part-of: falco-ecosystem
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ollama-init
        app.kubernetes.io/component: initialization
    spec:
      restartPolicy: OnFailure
      initContainers:
      - name: wait-for-ollama
        image: busybox:1.35
        command:
        - sh
        - -c
        - |
          echo "Waiting for Ollama service to be ready..."
          until wget -q --spider http://OLLAMA_SERVICE_NAME:11434/api/tags; do
            echo "Ollama not ready, waiting 10 seconds..."
            sleep 10
          done
          echo "Ollama is ready!"
      containers:
      - name: model-puller
        image: curlimages/curl:8.5.0
        command:
        - sh
        - -c
        - |
          echo "Pulling ultra-fast model: tinyllama"
          curl -X POST http://OLLAMA_SERVICE_NAME:11434/api/pull \
            -H "Content-Type: application/json" \
            -d '{"name": "tinyllama"}' \
            --max-time 1800
          
          echo "Pulling embedding model: nomic-embed-text"
          curl -X POST http://OLLAMA_SERVICE_NAME:11434/api/pull \
            -H "Content-Type: application/json" \
            -d '{"name": "nomic-embed-text"}' \
            --max-time 1800
          
          echo "Verifying models were pulled..."
          curl -s http://OLLAMA_SERVICE_NAME:11434/api/tags | grep -q "tinyllama" || exit 1
          curl -s http://OLLAMA_SERVICE_NAME:11434/api/tags | grep -q "nomic-embed-text" || exit 1
          
          echo "Warming up models with test inference..."
          curl -X POST http://OLLAMA_SERVICE_NAME:11434/api/generate \
            -H "Content-Type: application/json" \
            -d '{"model": "tinyllama", "prompt": "Hello, test", "stream": false}' \
            --max-time 60
          
          echo "Testing embedding model..."
          curl -X POST http://OLLAMA_SERVICE_NAME:11434/api/embeddings \
            -H "Content-Type: application/json" \
            -d '{"model": "nomic-embed-text", "prompt": "Hello, test"}' \
            --max-time 60
          
          echo "Model initialization and warm-up complete!"
        resources:
          requests:
            memory: "64Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "200m"
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000 