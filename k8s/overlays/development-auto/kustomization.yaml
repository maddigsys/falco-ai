apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

metadata:
  name: falco-ai-alerts-development-auto
  annotations:
    platform: gke
    storage-class: premium-rwo
    generated-at: "2025-07-15T02:57:51Z"

resources:
  - ../../base

namespace: falco-ai-alerts-development

namePrefix: dev-

commonLabels:
  environment: development
  platform: gke
  app.kubernetes.io/version: "2.0.0-development"

patches:
  # Platform-specific storage class
  - target:
      kind: PersistentVolumeClaim
      name: weaviate-pvc
    patch: |-
      - op: replace
        path: /spec/storageClassName
        value: premium-rwo
  
  # Platform-specific resource limits
  - target:
      kind: Deployment
      name: weaviate
    patch: |-
      - op: replace
        path: /spec/template/spec/containers/0/resources/requests/memory
        value: "256Mi"
      - op: replace
        path: /spec/template/spec/containers/0/resources/requests/cpu
        value: "100m"
      - op: replace
        path: /spec/template/spec/containers/0/resources/limits/memory
        value: "512Mi"
      - op: replace
        path: /spec/template/spec/containers/0/resources/limits/cpu
        value: "500m"
  
  # Environment-specific replicas
  - target:
      kind: Deployment
      name: falco-ai-alerts
    patch: |-
      - op: replace
        path: /spec/replicas
        value: 1
  
  # Environment-specific service URLs
  - target:
      kind: ConfigMap
      name: falco-ai-alerts-config
    patch: |-
      - op: replace
        path: /data/OLLAMA_API_URL
        value: "http://dev-ollama:11434/api/generate"
      - op: replace
        path: /data/WEAVIATE_HOST
        value: "dev-weaviate"
  
  # Environment-specific Ollama service name in init job
  - target:
      kind: Job
      name: ollama-model-init
    patch: |-
      - op: replace
        path: /spec/template/spec/initContainers/0/command/2
        value: |
          echo "Waiting for Ollama service to be ready..."
          until wget -q --spider http://dev-ollama:11434/api/tags; do
            echo "Ollama not ready, waiting 10 seconds..."
            sleep 10
          done
          echo "Ollama is ready!"
      - op: replace
        path: /spec/template/spec/containers/0/command/2
        value: |
          echo "Pulling ultra-fast model: tinyllama (~637MB)"
          curl -X POST http://dev-ollama:11434/api/pull             -H "Content-Type: application/json"             -d '{"name": "tinyllama"}'             --max-time 1800
          
          echo "Pulling embedding model: nomic-embed-text (~274MB)"
          curl -X POST http://dev-ollama:11434/api/pull             -H "Content-Type: application/json"             -d '{"name": "nomic-embed-text"}'             --max-time 1800
          
          echo "Verifying models were pulled..."
          curl -s http://dev-ollama:11434/api/tags | grep -q "tinyllama" || exit 1
          curl -s http://dev-ollama:11434/api/tags | grep -q "nomic-embed-text" || exit 1
          
          echo "Warming up models with test inference..."
          curl -X POST http://dev-ollama:11434/api/generate             -H "Content-Type: application/json"             -d '{"model": "tinyllama", "prompt": "Hello, test", "stream": false}'             --max-time 60
          
          echo "Testing embedding model..."
          curl -X POST http://dev-ollama:11434/api/embeddings             -H "Content-Type: application/json"             -d '{"model": "nomic-embed-text", "prompt": "Hello, test"}'             --max-time 60
          
          echo "Model initialization and warm-up complete!"

images:
  - name: falco-ai-alerts
    newName: maddigsys/falco-ai-alerts
    newTag: latest
